# Pipeline to run the HPU tests in DL1 Instance

trigger:
  tags:
    include: ["*"]
  branches:
    include:
      - "main"
      - "release/*"
      - "refs/tags/*"

pr:
  branches:
    include:
      - "main"
      - "release/*"

schedules:
- cron: '0 0 * * *'
  displayName: Daily midnight check
  branches:
    include: ["main"]

jobs:
  - job: testing
    # how long to run the job before automatically cancelling
    timeoutInMinutes: "35"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"
    strategy:
      matrix:
        'w. pytorch-lightning | pypi':
          image: "1.12.1/ubuntu22.04/habanalabs/pytorch-installer-2.0.1:latest"
          dependency: "pytorch-lightning"
          pkg_source: "pypi"
        'w. pytorch-lightning | source':
          image: "1.12.1/ubuntu22.04/habanalabs/pytorch-installer-2.0.1:latest"
          dependency: "pytorch-lightning"
          pkg_source: "source"
        'w. lightning | pypi':
          image: "1.12.1/ubuntu22.04/habanalabs/pytorch-installer-2.0.1:latest"
          dependency: "lightning"
          pkg_source: "pypi"
        'w. lightning | source':
          image: "1.12.1/ubuntu22.04/habanalabs/pytorch-installer-2.0.1:latest"
          dependency: "lightning"
          pkg_source: "source"
    pool: "intel-hpus"
    container:
      image: "vault.habana.ai/gaudi-docker/$(image)"
      options: "--runtime=habana \
                -e HABANA_VISIBLE_DEVICES=all \
                -e OMPI_MCA_btl_vader_single_copy_mechanism=none \
                --cap-add=sys_nice \
                --ipc=host \
                --shm-size=4g \
                -v /usr/bin/docker:/tmp/docker:ro"
    variables:
      DEVICES: $( python -c 'print("$(Agent.Name)".split("_")[-1])' )
      #MODULE_ID: $( python -c 'print("$(Agent.Name)".split("_")[-1])' )
      DEEPSPEED_VERSION: "1.12.1"
    workspace:
      clean: all

    steps:
    - bash: |
        echo "##vso[task.setvariable variable=HABANA_VISIBLE_DEVICES]$(DEVICES)"
        index_module_id=$(hl-smi -Q index,module_id -f csv)
        declare -A index_module_dict
        rows=($(echo "$index_module_id" | sed 's/, /,/g'))
        for row in "${rows[@]:1}"; do
            IFS=',' read -r index module_id <<< "$row"
            index_module_dict["$index"]=$module_id
        done
        IFS=', ' read -r -a keys_array <<< "$DEVICES"
        values=(); for key in "${keys_array[@]}"; do value="${index_module_dict[$key]}"; [ -n "$value" ] && values+=("$value"); done
        MODULE_ID=$(IFS=,; echo "${values[*]}")
        echo "##vso[task.setvariable variable=HABANA_VISIBLE_MODULES]$(MODULE_ID)"
      displayName: "set env. vars"

    - bash: |
        hl-smi
        lsmod | grep habanalabs
        echo "HABANA_VISIBLE_DEVICES=$HABANA_VISIBLE_DEVICES"
        echo "HABANA_VISIBLE_MODULES=$HABANA_VISIBLE_MODULES"
        hl-smi -Q index,module_id -f csv
        python --version
        pip --version
      displayName: 'Instance HW info'

    - bash: |
        set -ex
        pip install ".[$(dependency)]" -r requirements/_test.txt
        pip install git+https://github.com/HabanaAI/DeepSpeed.git@$(DEEPSPEED_VERSION)
      displayName: 'Install package & dependencies'

    - bash: pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip
      condition: eq(variables['pkg_source'], 'source')
      displayName: 'OverInstall lightning from source'

    - bash: pip uninstall -y pytorch-lightning
      condition: eq(variables['dependency'], 'lightning')
      displayName: 'drop PL package'

    - bash: pip uninstall -y lightning
      condition: eq(variables['dependency'], 'pytorch-lightning')
      displayName: 'drop Lightning package'

    # todo: add sanity check that needed cards are visible and accessible

    - bash: |
        pip list
        # todo: consider test all files not listed as you may easily forget to add new
        python -m pytest -sv \
          test_fabric/test_hpu_accelerator.py \
          test_fabric/test_hpu_strategy.py \
          test_pytorch/test_accelerator.py \
          test_pytorch/test_deepspeed.py \
          test_pytorch/test_hpu_graphs.py \
          test_pytorch/test_dynamic_shapes.py \
          --forked --junitxml=hpu_test-results.xml
      workingDirectory: tests/
      displayName: 'General HPU test'

    - bash: |
        python -m pytest -sv \
          test_pytorch/test_datamodule.py \
          test_pytorch/test_profiler.py \
          --forked --hpus 1 --junitxml=hpu1_test-results.xml
      workingDirectory: tests/
      displayName: 'Single card test'

    - bash: |
        set -ex
        python -m pytest -sv  test_pytorch/test_accelerator.py \
          --forked --hpus 2 --junitxml=hpu2_accel_test-results.xml
        python -m pytest -sv test_pytorch/test_profiler.py \
          --forked --hpus 2 --junitxml=hpu2_profiler_test-results.xml
      workingDirectory: tests/
      displayName: 'Multi card(2) HPU test'

    - bash: |
        LOWER_LIST=test_pytorch/ops_fp32.txt \
          FP32_LIST=test_pytorch/ops_bf16.txt \
          python -m pytest -sv test_pytorch/test_precision.py \
          -k test_autocast_operators_override --runxfail \
          --forked --junitxml=hpu_precision_test_override-results.xml
      workingDirectory: tests/
      displayName: 'HPU precision test'


    - bash: pip install ".[examples]"
      displayName: 'Install extra for examples'

    - bash: |
       export PYTHONPATH="${PYTHONPATH}:$(pwd)"
       python mnist_trainer.py
       LOWER_LIST=ops_fp32_mnist.txt FP32_LIST=ops_bf16_mnist.txt \
        python mnist_trainer.py -r autocast
       python hpu_graphs.py -v train --mode capture_and_replay make_graphed_callables modulecacher
       python hpu_graphs.py -v inference --mode capture_and_replay wrap_in_hpu_graph
       python hpu_graphs.py -v dynamicity --mode dynamic_control_flow dynamic_ops
      workingDirectory: examples/pytorch/
      displayName: 'Testing HPU examples'

    - task: PublishTestResults@2
      inputs:
        testResultsFiles: 'tests/*-results.xml'
        testRunTitle: '$(Build.DefinitionName) - Python $(python.version)'
      condition: succeededOrFailed()
      displayName: 'Publish test results'
